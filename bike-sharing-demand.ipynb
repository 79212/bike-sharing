{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM Approach\n",
    "\n",
    "CRISP-DM stands for Cross-Industry Standard Process for Data Mining and describes a structured approach to understanding, preparing, modeling and evaluating data. This process includes the following steps:\n",
    "\n",
    "1. Business Understanding\n",
    "2. Data Understanding\n",
    "3. Data Preparation\n",
    "4. Modeling\n",
    "5. Evaluation\n",
    "6. Deployment\n",
    "\n",
    "Deployment is not relevant for this type of project, as I won't be deploying my model to a web server. However, my results will be summarized in a blog post on Medium (https://medium.com/@julia.nikulski)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Business Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is similar to the bike sharing dataset provided by the [UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset). However, the aforementioned dataset only contains data for 2011 and 2012. It was originally created by Fanaee-T and Gama (2013) in their study [\"Event labeling combining ensemble detectors and background knowledge\"](https://link.springer.com/article/10.1007/s13748-013-0040-3). \n",
    "\n",
    "To have a larger dataset, I collected the following data from the following sources for the time period of January 1, 2011 until December 31, 2018:\n",
    "* The bike demand data comes from [Capital Bike Share](http://capitalbikeshare.com/system-data)\n",
    "* The weather data was taken from [NOAA's National Climatic Data Center](https://www.ncdc.noaa.gov/cdo-web/search)\n",
    "* The holiday data is from the [DC Department of Human Resources](http://dchr.dc.gov/page/holiday-schedule). \n",
    "\n",
    "The dataset uses data from the bike sharing stations of Capital Bike Share in Washington, D.C., USA. The weather data and holiday data refer to the same location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from scipy.stats import kruskal, pearsonr, randint, uniform, chi2_contingency, boxcox\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer, StandardScaler, power_transform\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, TimeSeriesSplit, RandomizedSearchCV, GridSearchCV, cross_val_predict\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller, kpss, acf, pacf\n",
    "from collections import defaultdict, OrderedDict\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.ar_model import AR\n",
    "from skits.feature_extraction import AutoregressiveTransformer\n",
    "from skits.preprocessing import ReversibleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diplaying all columns without truncation in dataframes\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Loading the data and visualizing and describing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>temp_observ</th>\n",
       "      <th>precip</th>\n",
       "      <th>wind</th>\n",
       "      <th>wt_fog</th>\n",
       "      <th>wt_heavy_fog</th>\n",
       "      <th>wt_thunder</th>\n",
       "      <th>wt_sleet</th>\n",
       "      <th>wt_hail</th>\n",
       "      <th>wt_glaze</th>\n",
       "      <th>wt_haze</th>\n",
       "      <th>wt_drift_snow</th>\n",
       "      <th>wt_high_wind</th>\n",
       "      <th>wt_mist</th>\n",
       "      <th>wt_drizzle</th>\n",
       "      <th>wt_rain</th>\n",
       "      <th>wt_freeze_rain</th>\n",
       "      <th>wt_snow</th>\n",
       "      <th>wt_ground_fog</th>\n",
       "      <th>wt_ice_fog</th>\n",
       "      <th>wt_freeze_drizzle</th>\n",
       "      <th>wt_unknown</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>total_cust</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.566667</td>\n",
       "      <td>11.973333</td>\n",
       "      <td>2.772727</td>\n",
       "      <td>0.069333</td>\n",
       "      <td>2.575</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>330.0</td>\n",
       "      <td>629.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>13.806667</td>\n",
       "      <td>7.327273</td>\n",
       "      <td>1.037349</td>\n",
       "      <td>3.925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.442857</td>\n",
       "      <td>7.464286</td>\n",
       "      <td>-3.060000</td>\n",
       "      <td>1.878824</td>\n",
       "      <td>3.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.957143</td>\n",
       "      <td>4.642857</td>\n",
       "      <td>-3.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1429.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.293333</td>\n",
       "      <td>6.113333</td>\n",
       "      <td>-1.772727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>1571.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  temp_avg  temp_min   temp_max  temp_observ    precip   wind  \\\n",
       "0  2011-01-01       NaN -1.566667  11.973333     2.772727  0.069333  2.575   \n",
       "1  2011-01-02       NaN  0.880000  13.806667     7.327273  1.037349  3.925   \n",
       "2  2011-01-03       NaN -3.442857   7.464286    -3.060000  1.878824  3.625   \n",
       "3  2011-01-04       NaN -5.957143   4.642857    -3.100000  0.000000  1.800   \n",
       "4  2011-01-05       NaN -4.293333   6.113333    -1.772727  0.000000  2.950   \n",
       "\n",
       "   wt_fog  wt_heavy_fog  wt_thunder  wt_sleet  wt_hail  wt_glaze  wt_haze  \\\n",
       "0     1.0           NaN         NaN       NaN      NaN       NaN      1.0   \n",
       "1     1.0           1.0         NaN       NaN      NaN       NaN      NaN   \n",
       "2     NaN           NaN         NaN       NaN      NaN       NaN      NaN   \n",
       "3     NaN           NaN         NaN       NaN      NaN       NaN      NaN   \n",
       "4     NaN           NaN         NaN       NaN      NaN       NaN      NaN   \n",
       "\n",
       "   wt_drift_snow  wt_high_wind  wt_mist  wt_drizzle  wt_rain  wt_freeze_rain  \\\n",
       "0            NaN           NaN      1.0         NaN      1.0             NaN   \n",
       "1            NaN           NaN      1.0         1.0      1.0             NaN   \n",
       "2            NaN           NaN      NaN         NaN      NaN             NaN   \n",
       "3            NaN           NaN      NaN         NaN      NaN             NaN   \n",
       "4            NaN           NaN      NaN         NaN      NaN             NaN   \n",
       "\n",
       "   wt_snow  wt_ground_fog  wt_ice_fog  wt_freeze_drizzle  wt_unknown  casual  \\\n",
       "0      NaN            NaN         NaN                NaN         NaN   330.0   \n",
       "1      NaN            NaN         NaN                NaN         NaN   130.0   \n",
       "2      NaN            NaN         NaN                NaN         NaN   120.0   \n",
       "3      NaN            NaN         NaN                NaN         NaN   107.0   \n",
       "4      NaN            NaN         NaN                NaN         NaN    82.0   \n",
       "\n",
       "   registered  total_cust  holiday  \n",
       "0       629.0       959.0      NaN  \n",
       "1       651.0       781.0      NaN  \n",
       "2      1181.0      1301.0      NaN  \n",
       "3      1429.0      1536.0      NaN  \n",
       "4      1489.0      1571.0      NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in bike sharing dataset\n",
    "bike_df = pd.read_csv('bike_sharing_dataset.csv')\n",
    "bike_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>temp_observ</th>\n",
       "      <th>precip</th>\n",
       "      <th>wind</th>\n",
       "      <th>wt_fog</th>\n",
       "      <th>wt_heavy_fog</th>\n",
       "      <th>wt_thunder</th>\n",
       "      <th>wt_sleet</th>\n",
       "      <th>wt_hail</th>\n",
       "      <th>wt_glaze</th>\n",
       "      <th>wt_haze</th>\n",
       "      <th>wt_drift_snow</th>\n",
       "      <th>wt_high_wind</th>\n",
       "      <th>wt_mist</th>\n",
       "      <th>wt_drizzle</th>\n",
       "      <th>wt_rain</th>\n",
       "      <th>wt_freeze_rain</th>\n",
       "      <th>wt_snow</th>\n",
       "      <th>wt_ground_fog</th>\n",
       "      <th>wt_ice_fog</th>\n",
       "      <th>wt_freeze_drizzle</th>\n",
       "      <th>wt_unknown</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>total_cust</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2101.000000</td>\n",
       "      <td>2922.000000</td>\n",
       "      <td>2922.000000</td>\n",
       "      <td>2922.000000</td>\n",
       "      <td>2922.000000</td>\n",
       "      <td>2922.000000</td>\n",
       "      <td>1503.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2918.000000</td>\n",
       "      <td>2918.000000</td>\n",
       "      <td>2918.000000</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.419007</td>\n",
       "      <td>8.506468</td>\n",
       "      <td>19.015689</td>\n",
       "      <td>11.069243</td>\n",
       "      <td>3.435734</td>\n",
       "      <td>3.162898</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1679.776217</td>\n",
       "      <td>6046.297121</td>\n",
       "      <td>7726.073338</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.556401</td>\n",
       "      <td>9.473941</td>\n",
       "      <td>9.835524</td>\n",
       "      <td>9.481232</td>\n",
       "      <td>8.183658</td>\n",
       "      <td>1.379582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1560.762932</td>\n",
       "      <td>2756.888032</td>\n",
       "      <td>3745.220092</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-12.100000</td>\n",
       "      <td>-16.993750</td>\n",
       "      <td>-7.980000</td>\n",
       "      <td>-15.658333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.566667</td>\n",
       "      <td>0.516538</td>\n",
       "      <td>11.081562</td>\n",
       "      <td>3.013068</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>512.250000</td>\n",
       "      <td>3839.250000</td>\n",
       "      <td>4628.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15.433333</td>\n",
       "      <td>8.504911</td>\n",
       "      <td>19.992857</td>\n",
       "      <td>11.619091</td>\n",
       "      <td>0.271504</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1220.500000</td>\n",
       "      <td>5964.000000</td>\n",
       "      <td>7442.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.066667</td>\n",
       "      <td>17.338393</td>\n",
       "      <td>27.874583</td>\n",
       "      <td>19.767083</td>\n",
       "      <td>2.885381</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2357.250000</td>\n",
       "      <td>8187.500000</td>\n",
       "      <td>10849.500000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31.733333</td>\n",
       "      <td>26.206250</td>\n",
       "      <td>37.850000</td>\n",
       "      <td>28.666667</td>\n",
       "      <td>118.789796</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10173.000000</td>\n",
       "      <td>15419.000000</td>\n",
       "      <td>19113.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          temp_avg     temp_min     temp_max  temp_observ       precip  \\\n",
       "count  2101.000000  2922.000000  2922.000000  2922.000000  2922.000000   \n",
       "mean     14.419007     8.506468    19.015689    11.069243     3.435734   \n",
       "std       9.556401     9.473941     9.835524     9.481232     8.183658   \n",
       "min     -12.100000   -16.993750    -7.980000   -15.658333     0.000000   \n",
       "25%       6.566667     0.516538    11.081562     3.013068     0.005510   \n",
       "50%      15.433333     8.504911    19.992857    11.619091     0.271504   \n",
       "75%      23.066667    17.338393    27.874583    19.767083     2.885381   \n",
       "max      31.733333    26.206250    37.850000    28.666667   118.789796   \n",
       "\n",
       "              wind  wt_fog  wt_heavy_fog  wt_thunder  wt_sleet  wt_hail  \\\n",
       "count  2922.000000  1503.0         208.0       694.0     129.0     50.0   \n",
       "mean      3.162898     1.0           1.0         1.0       1.0      1.0   \n",
       "std       1.379582     0.0           0.0         0.0       0.0      0.0   \n",
       "min       0.375000     1.0           1.0         1.0       1.0      1.0   \n",
       "25%       2.200000     1.0           1.0         1.0       1.0      1.0   \n",
       "50%       2.900000     1.0           1.0         1.0       1.0      1.0   \n",
       "75%       3.875000     1.0           1.0         1.0       1.0      1.0   \n",
       "max      12.750000     1.0           1.0         1.0       1.0      1.0   \n",
       "\n",
       "       wt_glaze  wt_haze  wt_drift_snow  wt_high_wind  wt_mist  wt_drizzle  \\\n",
       "count     153.0    705.0            7.0         258.0    371.0       128.0   \n",
       "mean        1.0      1.0            1.0           1.0      1.0         1.0   \n",
       "std         0.0      0.0            0.0           0.0      0.0         0.0   \n",
       "min         1.0      1.0            1.0           1.0      1.0         1.0   \n",
       "25%         1.0      1.0            1.0           1.0      1.0         1.0   \n",
       "50%         1.0      1.0            1.0           1.0      1.0         1.0   \n",
       "75%         1.0      1.0            1.0           1.0      1.0         1.0   \n",
       "max         1.0      1.0            1.0           1.0      1.0         1.0   \n",
       "\n",
       "       wt_rain  wt_freeze_rain  wt_snow  wt_ground_fog  wt_ice_fog  \\\n",
       "count    406.0             5.0     84.0           36.0        10.0   \n",
       "mean       1.0             1.0      1.0            1.0         1.0   \n",
       "std        0.0             0.0      0.0            0.0         0.0   \n",
       "min        1.0             1.0      1.0            1.0         1.0   \n",
       "25%        1.0             1.0      1.0            1.0         1.0   \n",
       "50%        1.0             1.0      1.0            1.0         1.0   \n",
       "75%        1.0             1.0      1.0            1.0         1.0   \n",
       "max        1.0             1.0      1.0            1.0         1.0   \n",
       "\n",
       "       wt_freeze_drizzle  wt_unknown        casual    registered  \\\n",
       "count                4.0         1.0   2918.000000   2918.000000   \n",
       "mean                 1.0         1.0   1679.776217   6046.297121   \n",
       "std                  0.0         NaN   1560.762932   2756.888032   \n",
       "min                  1.0         1.0      2.000000     19.000000   \n",
       "25%                  1.0         1.0    512.250000   3839.250000   \n",
       "50%                  1.0         1.0   1220.500000   5964.000000   \n",
       "75%                  1.0         1.0   2357.250000   8187.500000   \n",
       "max                  1.0         1.0  10173.000000  15419.000000   \n",
       "\n",
       "         total_cust  holiday  \n",
       "count   2918.000000     89.0  \n",
       "mean    7726.073338      1.0  \n",
       "std     3745.220092      0.0  \n",
       "min       21.000000      1.0  \n",
       "25%     4628.500000      1.0  \n",
       "50%     7442.500000      1.0  \n",
       "75%    10849.500000      1.0  \n",
       "max    19113.000000      1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print descriptive statistics\n",
    "bike_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                  object\n",
       "temp_avg             float64\n",
       "temp_min             float64\n",
       "temp_max             float64\n",
       "temp_observ          float64\n",
       "precip               float64\n",
       "wind                 float64\n",
       "wt_fog               float64\n",
       "wt_heavy_fog         float64\n",
       "wt_thunder           float64\n",
       "wt_sleet             float64\n",
       "wt_hail              float64\n",
       "wt_glaze             float64\n",
       "wt_haze              float64\n",
       "wt_drift_snow        float64\n",
       "wt_high_wind         float64\n",
       "wt_mist              float64\n",
       "wt_drizzle           float64\n",
       "wt_rain              float64\n",
       "wt_freeze_rain       float64\n",
       "wt_snow              float64\n",
       "wt_ground_fog        float64\n",
       "wt_ice_fog           float64\n",
       "wt_freeze_drizzle    float64\n",
       "wt_unknown           float64\n",
       "casual               float64\n",
       "registered           float64\n",
       "total_cust           float64\n",
       "holiday              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the datatypes of each variable\n",
    "bike_df.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the source repository of the data set, these are the meanings of each variable:\n",
    "- date: date\n",
    "- temp_avg: average temperature in Celcius\n",
    "- temp_min: minimum temperature in Celcius\n",
    "- temp_max: maximum temperature in Celcius\n",
    "- temp_observ: temperature at time of observation\n",
    "- precip: precipitation in mm \n",
    "- wind: average windspeed\n",
    "- wt: weather types\n",
    "    - fog: fog, ice fog, or freezing fog (may include heavy fog)\n",
    "    - heavy_fog: heavy fog or heaving freezing fog (not always distinguished from fog)\n",
    "    - thunder: thunder\n",
    "    - sleet: ice pellets, sleet, snow pellets, or small hail\n",
    "    - hail: hail (may include small hail)\n",
    "    - glaze: glaze or rime\n",
    "    - haze: smoke or haze\n",
    "    - drift_snow: blowing or drifting snow \n",
    "    - high_wind: high or damaging winds\n",
    "    - mist: mist\n",
    "    - drizzle: drizzle\n",
    "    - rain: rain (may include freezing rain, drizzle, and freezing drizzle)\n",
    "    - freeze_rain: freezing rain\n",
    "    - snow: snow, snow pellets, snow grains, or ice crystals\n",
    "    - ground_fog: ground fog\n",
    "    - ice_fog: ice for or freezing fog\n",
    "    - freeze_drizzle: freezing drizzle\n",
    "    - unknown: unknown source of precipitation\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- total_cust: count of total rental bikes including both casual and registered\n",
    "- holiday: holiday in Washington D.C.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for and dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                    0\n",
       "temp_avg              821\n",
       "temp_min                0\n",
       "temp_max                0\n",
       "temp_observ             0\n",
       "precip                  0\n",
       "wind                    0\n",
       "wt_fog               1419\n",
       "wt_heavy_fog         2714\n",
       "wt_thunder           2228\n",
       "wt_sleet             2793\n",
       "wt_hail              2872\n",
       "wt_glaze             2769\n",
       "wt_haze              2217\n",
       "wt_drift_snow        2915\n",
       "wt_high_wind         2664\n",
       "wt_mist              2551\n",
       "wt_drizzle           2794\n",
       "wt_rain              2516\n",
       "wt_freeze_rain       2917\n",
       "wt_snow              2838\n",
       "wt_ground_fog        2886\n",
       "wt_ice_fog           2912\n",
       "wt_freeze_drizzle    2918\n",
       "wt_unknown           2921\n",
       "casual                  4\n",
       "registered              4\n",
       "total_cust              4\n",
       "holiday              2833\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "bike_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature **temp_avg** is missing 821 values. I will use the temp_min, temp_max and temp_observ features to estimate the missing temp_avg values. There are also 4 values missing for **casual, registered and total_cust features** and the target, respectively. I will check why these are missing and exclude those observations because if the target variable is missing, I cannot estimate it. \n",
    "\n",
    "The **holiday** feature has NAs but these should actually 0s, which I will convert. The **wt_** features also have NAs where 0s should be which I will insert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NAs with 0 where applicable\n",
    "wt_feats = [x for x in bike_df.columns if 'wt' in x]\n",
    "bike_df['holiday'] = bike_df['holiday'].fillna(0)\n",
    "bike_df[wt_feats] = bike_df[wt_feats].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be four days where no data was captured for the rented bikes. Because this is a time series, I am not sure whether I can drop these rows or whether I should interpolate the target values for these four days. More research is necessary to determine this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>temp_observ</th>\n",
       "      <th>precip</th>\n",
       "      <th>wind</th>\n",
       "      <th>wt_fog</th>\n",
       "      <th>wt_heavy_fog</th>\n",
       "      <th>wt_thunder</th>\n",
       "      <th>wt_sleet</th>\n",
       "      <th>wt_hail</th>\n",
       "      <th>wt_glaze</th>\n",
       "      <th>wt_haze</th>\n",
       "      <th>wt_drift_snow</th>\n",
       "      <th>wt_high_wind</th>\n",
       "      <th>wt_mist</th>\n",
       "      <th>wt_drizzle</th>\n",
       "      <th>wt_rain</th>\n",
       "      <th>wt_freeze_rain</th>\n",
       "      <th>wt_snow</th>\n",
       "      <th>wt_ground_fog</th>\n",
       "      <th>wt_ice_fog</th>\n",
       "      <th>wt_freeze_drizzle</th>\n",
       "      <th>wt_unknown</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>total_cust</th>\n",
       "      <th>holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>2016-01-23</td>\n",
       "      <td>-4.366667</td>\n",
       "      <td>-6.128571</td>\n",
       "      <td>-2.392857</td>\n",
       "      <td>-4.688889</td>\n",
       "      <td>42.045946</td>\n",
       "      <td>8.08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>2016-01-24</td>\n",
       "      <td>-2.666667</td>\n",
       "      <td>-7.985714</td>\n",
       "      <td>-1.028571</td>\n",
       "      <td>-6.366667</td>\n",
       "      <td>19.339130</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>2016-01-25</td>\n",
       "      <td>-5.133333</td>\n",
       "      <td>-11.128571</td>\n",
       "      <td>2.028571</td>\n",
       "      <td>-9.877778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>-7.871429</td>\n",
       "      <td>7.471429</td>\n",
       "      <td>3.588889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  temp_avg   temp_min  temp_max  temp_observ     precip  wind  \\\n",
       "1848  2016-01-23 -4.366667  -6.128571 -2.392857    -4.688889  42.045946  8.08   \n",
       "1849  2016-01-24 -2.666667  -7.985714 -1.028571    -6.366667  19.339130  3.75   \n",
       "1850  2016-01-25 -5.133333 -11.128571  2.028571    -9.877778   0.000000  1.15   \n",
       "1851  2016-01-26  2.333333  -7.871429  7.471429     3.588889   0.000000  2.85   \n",
       "\n",
       "      wt_fog  wt_heavy_fog  wt_thunder  wt_sleet  wt_hail  wt_glaze  wt_haze  \\\n",
       "1848     1.0           1.0         1.0       1.0      0.0       0.0      0.0   \n",
       "1849     1.0           0.0         0.0       0.0      0.0       1.0      1.0   \n",
       "1850     1.0           0.0         0.0       0.0      0.0       1.0      1.0   \n",
       "1851     1.0           0.0         0.0       0.0      0.0       1.0      1.0   \n",
       "\n",
       "      wt_drift_snow  wt_high_wind  wt_mist  wt_drizzle  wt_rain  \\\n",
       "1848            1.0           1.0      0.0         0.0      0.0   \n",
       "1849            1.0           1.0      0.0         0.0      0.0   \n",
       "1850            0.0           0.0      0.0         0.0      0.0   \n",
       "1851            0.0           0.0      0.0         0.0      0.0   \n",
       "\n",
       "      wt_freeze_rain  wt_snow  wt_ground_fog  wt_ice_fog  wt_freeze_drizzle  \\\n",
       "1848             0.0      0.0            0.0         0.0                0.0   \n",
       "1849             0.0      0.0            0.0         0.0                0.0   \n",
       "1850             0.0      0.0            0.0         0.0                0.0   \n",
       "1851             0.0      0.0            0.0         0.0                0.0   \n",
       "\n",
       "      wt_unknown  casual  registered  total_cust  holiday  \n",
       "1848         0.0     NaN         NaN         NaN      0.0  \n",
       "1849         0.0     NaN         NaN         NaN      0.0  \n",
       "1850         0.0     NaN         NaN         NaN      0.0  \n",
       "1851         0.0     NaN         NaN         NaN      0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check casual, registered and total_cust missing rows\n",
    "missing_target = bike_df[bike_df['total_cust'].isna()]\n",
    "missing_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                   0\n",
       "temp_avg             821\n",
       "temp_min               0\n",
       "temp_max               0\n",
       "temp_observ            0\n",
       "precip                 0\n",
       "wind                   0\n",
       "wt_fog                 0\n",
       "wt_heavy_fog           0\n",
       "wt_thunder             0\n",
       "wt_sleet               0\n",
       "wt_hail                0\n",
       "wt_glaze               0\n",
       "wt_haze                0\n",
       "wt_drift_snow          0\n",
       "wt_high_wind           0\n",
       "wt_mist                0\n",
       "wt_drizzle             0\n",
       "wt_rain                0\n",
       "wt_freeze_rain         0\n",
       "wt_snow                0\n",
       "wt_ground_fog          0\n",
       "wt_ice_fog             0\n",
       "wt_freeze_drizzle      0\n",
       "wt_unknown             0\n",
       "casual                 0\n",
       "registered             0\n",
       "total_cust             0\n",
       "holiday                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filling the missing values in the customer variables with forward fill method\n",
    "bike_df[['total_cust', 'casual', 'registered']] = bike_df[['total_cust', 'casual', 'registered']].fillna(\n",
    "                                                            method='ffill')\n",
    "bike_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_avg: (0.7289653483071338, 0.0)\n",
      "temp_min: (0.5484063784007367, 3.9226572899081034e-229)\n",
      "temp_min, without first 820 rows: (0.6720561174778342, 3.520489340545429e-276)\n",
      "temp_max: (0.5962507437902363, 6.98668965923709e-281)\n",
      "temp_max, without first 820 rows: (0.7406902128448435, 0.0)\n",
      "temp_observ: (0.5500110613450738, 9.817169026469553e-231)\n",
      "temp_observ, without first 820 rows: (0.6804926361142356, 9.04108786780588e-286)\n"
     ]
    }
   ],
   "source": [
    "# check what the correlation between the different temperature features and total_cust is\n",
    "# maybe I could just use one of the other temperature features instead of temp_avg\n",
    "\n",
    "# correlation between temp_avg and total_cust\n",
    "# I'm excluding the first 820 rows because they contain missing temp_avg values\n",
    "print('temp_avg:', pearsonr(bike_df['temp_avg'][821:], bike_df['total_cust'][821:]))\n",
    "\n",
    "# correlation between temp_min and total_cust\n",
    "print('temp_min:', pearsonr(bike_df['temp_min'], bike_df['total_cust']))\n",
    "print('temp_min, without first 820 rows:', pearsonr(bike_df['temp_min'][821:], bike_df['total_cust'][821:]))\n",
    "\n",
    "# correlation between temp_max and total_cust\n",
    "print('temp_max:', pearsonr(bike_df['temp_max'], bike_df['total_cust']))\n",
    "print('temp_max, without first 820 rows:', pearsonr(bike_df['temp_max'][821:], bike_df['total_cust'][821:]))\n",
    "\n",
    "# correlation between temp_observ and total_cust\n",
    "print('temp_observ:', pearsonr(bike_df['temp_observ'], bike_df['total_cust']))\n",
    "print('temp_observ, without first 820 rows:', pearsonr(bike_df['temp_observ'][821:], bike_df['total_cust'][821:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that disregarding the first 820 rows leads to the highest correlation between temp_max and total_cust. Comparing temp_avg with the entire dataset and the respective correlations with total_cust, temp_avg has the highest correlation.\n",
    "\n",
    "The Granger causality is a more appropriate measure for timeseries data and understanding how one variable can determine another variable. Thus, below I am using the Granger causality test to see how the temperature features cause total_cust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=176.4496, p=0.0000  , df_denom=2097, df_num=1\n",
      "ssr based chi2 test:   chi2=176.7020, p=0.0000  , df=1\n",
      "likelihood ratio test: chi2=169.6602, p=0.0000  , df=1\n",
      "parameter F test:         F=176.4496, p=0.0000  , df_denom=2097, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 2\n",
      "ssr based F test:         F=57.9079 , p=0.0000  , df_denom=2094, df_num=2\n",
      "ssr based chi2 test:   chi2=116.0923, p=0.0000  , df=2\n",
      "likelihood ratio test: chi2=112.9955, p=0.0000  , df=2\n",
      "parameter F test:         F=57.9079 , p=0.0000  , df_denom=2094, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 3\n",
      "ssr based F test:         F=29.7872 , p=0.0000  , df_denom=2091, df_num=3\n",
      "ssr based chi2 test:   chi2=89.6607 , p=0.0000  , df=3\n",
      "likelihood ratio test: chi2=87.7977 , p=0.0000  , df=3\n",
      "parameter F test:         F=29.7872 , p=0.0000  , df_denom=2091, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 4\n",
      "ssr based F test:         F=16.7444 , p=0.0000  , df_denom=2088, df_num=4\n",
      "ssr based chi2 test:   chi2=67.2664 , p=0.0000  , df=4\n",
      "likelihood ratio test: chi2=66.2100 , p=0.0000  , df=4\n",
      "parameter F test:         F=16.7444 , p=0.0000  , df_denom=2088, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 5\n",
      "ssr based F test:         F=10.1752 , p=0.0000  , df_denom=2085, df_num=5\n",
      "ssr based chi2 test:   chi2=51.1447 , p=0.0000  , df=5\n",
      "likelihood ratio test: chi2=50.5306 , p=0.0000  , df=5\n",
      "parameter F test:         F=10.1752 , p=0.0000  , df_denom=2085, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 6\n",
      "ssr based F test:         F=7.9797  , p=0.0000  , df_denom=2082, df_num=6\n",
      "ssr based chi2 test:   chi2=48.1769 , p=0.0000  , df=6\n",
      "likelihood ratio test: chi2=47.6313 , p=0.0000  , df=6\n",
      "parameter F test:         F=7.9797  , p=0.0000  , df_denom=2082, df_num=6\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 7\n",
      "ssr based F test:         F=5.7921  , p=0.0000  , df_denom=2079, df_num=7\n",
      "ssr based chi2 test:   chi2=40.8369 , p=0.0000  , df=7\n",
      "likelihood ratio test: chi2=40.4438 , p=0.0000  , df=7\n",
      "parameter F test:         F=5.7921  , p=0.0000  , df_denom=2079, df_num=7\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=40.0233 , p=0.0000  , df_denom=2918, df_num=1\n",
      "ssr based chi2 test:   chi2=40.0645 , p=0.0000  , df=1\n",
      "likelihood ratio test: chi2=39.7922 , p=0.0000  , df=1\n",
      "parameter F test:         F=40.0233 , p=0.0000  , df_denom=2918, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 2\n",
      "ssr based F test:         F=9.9974  , p=0.0000  , df_denom=2915, df_num=2\n",
      "ssr based chi2 test:   chi2=20.0290 , p=0.0000  , df=2\n",
      "likelihood ratio test: chi2=19.9606 , p=0.0000  , df=2\n",
      "parameter F test:         F=9.9974  , p=0.0000  , df_denom=2915, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 3\n",
      "ssr based F test:         F=13.1394 , p=0.0000  , df_denom=2912, df_num=3\n",
      "ssr based chi2 test:   chi2=39.5130 , p=0.0000  , df=3\n",
      "likelihood ratio test: chi2=39.2480 , p=0.0000  , df=3\n",
      "parameter F test:         F=13.1394 , p=0.0000  , df_denom=2912, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 4\n",
      "ssr based F test:         F=4.6505  , p=0.0010  , df_denom=2909, df_num=4\n",
      "ssr based chi2 test:   chi2=18.6596 , p=0.0009  , df=4\n",
      "likelihood ratio test: chi2=18.6002 , p=0.0009  , df=4\n",
      "parameter F test:         F=4.6505  , p=0.0010  , df_denom=2909, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 5\n",
      "ssr based F test:         F=5.3872  , p=0.0001  , df_denom=2906, df_num=5\n",
      "ssr based chi2 test:   chi2=27.0378 , p=0.0001  , df=5\n",
      "likelihood ratio test: chi2=26.9133 , p=0.0001  , df=5\n",
      "parameter F test:         F=5.3872  , p=0.0001  , df_denom=2906, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 6\n",
      "ssr based F test:         F=4.5610  , p=0.0001  , df_denom=2903, df_num=6\n",
      "ssr based chi2 test:   chi2=27.4884 , p=0.0001  , df=6\n",
      "likelihood ratio test: chi2=27.3597 , p=0.0001  , df=6\n",
      "parameter F test:         F=4.5610  , p=0.0001  , df_denom=2903, df_num=6\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 7\n",
      "ssr based F test:         F=3.3295  , p=0.0016  , df_denom=2900, df_num=7\n",
      "ssr based chi2 test:   chi2=23.4271 , p=0.0014  , df=7\n",
      "likelihood ratio test: chi2=23.3335 , p=0.0015  , df=7\n",
      "parameter F test:         F=3.3295  , p=0.0016  , df_denom=2900, df_num=7\n"
     ]
    }
   ],
   "source": [
    "grangercausalitytests(bike_df[['total_cust', 'temp_avg']][821:], maxlag=7);\n",
    "grangercausalitytests(bike_df[['total_cust', 'temp_max']], maxlag=7);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating dataframe with timeseries for entire DC metro area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will engineer some new features from the categorical and numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create seasons for dataframe\n",
    "def seasons(df):\n",
    "    '''\n",
    "    Function to create new features for seasons based on months\n",
    "    Args: df = dataframe\n",
    "    Returns: df = dataframe\n",
    "    '''\n",
    "    # create a season features\n",
    "    df['season_spring'] = df['date'].apply(lambda x: 1 if '01' in x[5:7] else 1 if '02' in x[5:7] else 1 \n",
    "                                                     if '03' in x[5:7] else 0)\n",
    "    df['season_summer'] = df['date'].apply(lambda x: 1 if '04' in x[5:7] else 1 if '05' in x[5:7] else 1 \n",
    "                                                     if '06' in x[5:7] else 0)\n",
    "    df['season_fall'] = df['date'].apply(lambda x: 1 if '07' in x[5:7] else 1 if '08' in x[5:7] else 1 \n",
    "                                                     if '09' in x[5:7] else 0)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create new features for seasons\n",
    "bike_df = seasons(bike_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create new feature weekday\n",
    "bike_df['date_datetime'] = bike_df['date'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\"))\n",
    "\n",
    "bike_df['weekday'] = bike_df['date_datetime'].apply(lambda x: x.weekday())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create month feature\n",
    "bike_df['month'] = bike_df['date_datetime'].apply(lambda x: x.month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### one hot encode the feature month\n",
    "month_dummies = pd.get_dummies(bike_df['month'], prefix='month', drop_first=True)\n",
    "bike_df = bike_df.join(month_dummies, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>temp_observ</th>\n",
       "      <th>precip</th>\n",
       "      <th>wind</th>\n",
       "      <th>wt_fog</th>\n",
       "      <th>wt_heavy_fog</th>\n",
       "      <th>wt_thunder</th>\n",
       "      <th>wt_sleet</th>\n",
       "      <th>wt_hail</th>\n",
       "      <th>wt_glaze</th>\n",
       "      <th>wt_haze</th>\n",
       "      <th>wt_drift_snow</th>\n",
       "      <th>wt_high_wind</th>\n",
       "      <th>wt_mist</th>\n",
       "      <th>wt_drizzle</th>\n",
       "      <th>wt_rain</th>\n",
       "      <th>wt_freeze_rain</th>\n",
       "      <th>wt_snow</th>\n",
       "      <th>wt_ground_fog</th>\n",
       "      <th>wt_ice_fog</th>\n",
       "      <th>wt_freeze_drizzle</th>\n",
       "      <th>wt_unknown</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>total_cust</th>\n",
       "      <th>holiday</th>\n",
       "      <th>season_spring</th>\n",
       "      <th>season_summer</th>\n",
       "      <th>season_fall</th>\n",
       "      <th>date_datetime</th>\n",
       "      <th>weekday</th>\n",
       "      <th>month</th>\n",
       "      <th>month_2</th>\n",
       "      <th>month_3</th>\n",
       "      <th>month_4</th>\n",
       "      <th>month_5</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.566667</td>\n",
       "      <td>11.973333</td>\n",
       "      <td>2.772727</td>\n",
       "      <td>0.069333</td>\n",
       "      <td>2.575</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>629.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>13.806667</td>\n",
       "      <td>7.327273</td>\n",
       "      <td>1.037349</td>\n",
       "      <td>3.925</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>781.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.442857</td>\n",
       "      <td>7.464286</td>\n",
       "      <td>-3.060000</td>\n",
       "      <td>1.878824</td>\n",
       "      <td>3.625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>1301.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.957143</td>\n",
       "      <td>4.642857</td>\n",
       "      <td>-3.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1429.0</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.293333</td>\n",
       "      <td>6.113333</td>\n",
       "      <td>-1.772727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>1571.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  temp_avg  temp_min   temp_max  temp_observ    precip   wind  \\\n",
       "0  2011-01-01       NaN -1.566667  11.973333     2.772727  0.069333  2.575   \n",
       "1  2011-01-02       NaN  0.880000  13.806667     7.327273  1.037349  3.925   \n",
       "2  2011-01-03       NaN -3.442857   7.464286    -3.060000  1.878824  3.625   \n",
       "3  2011-01-04       NaN -5.957143   4.642857    -3.100000  0.000000  1.800   \n",
       "4  2011-01-05       NaN -4.293333   6.113333    -1.772727  0.000000  2.950   \n",
       "\n",
       "   wt_fog  wt_heavy_fog  wt_thunder  wt_sleet  wt_hail  wt_glaze  wt_haze  \\\n",
       "0     1.0           0.0         0.0       0.0      0.0       0.0      1.0   \n",
       "1     1.0           1.0         0.0       0.0      0.0       0.0      0.0   \n",
       "2     0.0           0.0         0.0       0.0      0.0       0.0      0.0   \n",
       "3     0.0           0.0         0.0       0.0      0.0       0.0      0.0   \n",
       "4     0.0           0.0         0.0       0.0      0.0       0.0      0.0   \n",
       "\n",
       "   wt_drift_snow  wt_high_wind  wt_mist  wt_drizzle  wt_rain  wt_freeze_rain  \\\n",
       "0            0.0           0.0      1.0         0.0      1.0             0.0   \n",
       "1            0.0           0.0      1.0         1.0      1.0             0.0   \n",
       "2            0.0           0.0      0.0         0.0      0.0             0.0   \n",
       "3            0.0           0.0      0.0         0.0      0.0             0.0   \n",
       "4            0.0           0.0      0.0         0.0      0.0             0.0   \n",
       "\n",
       "   wt_snow  wt_ground_fog  wt_ice_fog  wt_freeze_drizzle  wt_unknown  casual  \\\n",
       "0      0.0            0.0         0.0                0.0         0.0   330.0   \n",
       "1      0.0            0.0         0.0                0.0         0.0   130.0   \n",
       "2      0.0            0.0         0.0                0.0         0.0   120.0   \n",
       "3      0.0            0.0         0.0                0.0         0.0   107.0   \n",
       "4      0.0            0.0         0.0                0.0         0.0    82.0   \n",
       "\n",
       "   registered  total_cust  holiday  season_spring  season_summer  season_fall  \\\n",
       "0       629.0       959.0      0.0              1              0            0   \n",
       "1       651.0       781.0      0.0              1              0            0   \n",
       "2      1181.0      1301.0      0.0              1              0            0   \n",
       "3      1429.0      1536.0      0.0              1              0            0   \n",
       "4      1489.0      1571.0      0.0              1              0            0   \n",
       "\n",
       "  date_datetime  weekday  month  month_2  month_3  month_4  month_5  month_6  \\\n",
       "0    2011-01-01        5      1        0        0        0        0        0   \n",
       "1    2011-01-02        6      1        0        0        0        0        0   \n",
       "2    2011-01-03        0      1        0        0        0        0        0   \n",
       "3    2011-01-04        1      1        0        0        0        0        0   \n",
       "4    2011-01-05        2      1        0        0        0        0        0   \n",
       "\n",
       "   month_7  month_8  month_9  month_10  month_11  month_12  weekday_1  \\\n",
       "0        0        0        0         0         0         0          0   \n",
       "1        0        0        0         0         0         0          0   \n",
       "2        0        0        0         0         0         0          0   \n",
       "3        0        0        0         0         0         0          1   \n",
       "4        0        0        0         0         0         0          0   \n",
       "\n",
       "   weekday_2  weekday_3  weekday_4  weekday_5  weekday_6  \n",
       "0          0          0          0          1          0  \n",
       "1          0          0          0          0          1  \n",
       "2          0          0          0          0          0  \n",
       "3          0          0          0          0          0  \n",
       "4          1          0          0          0          0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### one hot encode the feature weekday\n",
    "weekday_dummies = pd.get_dummies(bike_df['weekday'], prefix='weekday', drop_first=True)\n",
    "bike_df = bike_df.join(weekday_dummies, how='left')\n",
    "bike_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create new feature working_day\n",
    "bike_df['working_day'] = bike_df['weekday'].apply(lambda x: 0 if x > 5 or x == 0 else 1)\n",
    "bike_df['working_day'] = bike_df[['holiday', 'working_day']].apply(\n",
    "    lambda x: 0 if x['holiday'] == 1 else x['working_day'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropping unnecessary columns\n",
    "\n",
    "I will drop the date feature and keep the date_datetime feature to have the feature with the correct date formatting. The registered and casual features will also be dropped because they are sub-features of the total_cust target label. I will also drop the temp_avg variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping date, registered and casual features because this is in string format \n",
    "bike_df.drop(columns=['date', 'temp_avg', 'registered', 'casual'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any non-categorical variables\n",
    "bike_df_corr_cat = bike_df.drop(columns=['date_datetime', 'weekday', 'temp_min', 'temp_max',\n",
    "                                         'temp_observ', 'precip', 'wind', 'total_cust', 'month'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code snippet was taken from https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create correlation matrix with cramer's V coefficients\n",
    "corr_matrix = pd.DataFrame(data = None, index=np.arange(len(bike_df_corr_cat.columns)), \n",
    "                            columns=bike_df_corr_cat.columns)\n",
    "\n",
    "for col in bike_df_corr_cat.columns:\n",
    "    count = 0\n",
    "    for val in bike_df_corr_cat.columns:\n",
    "        corr_cat = cramers_v(bike_df_corr_cat[col], bike_df_corr_cat[val])\n",
    "        corr_matrix[col][count] = corr_cat\n",
    "        count += 1\n",
    "    corr_matrix = corr_matrix.astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add an index to the dataframe\n",
    "corr_matrix['columns'] = bike_df_corr_cat.columns\n",
    "corr_matrix.set_index('columns', inplace=True)\n",
    "\n",
    "# plot a heatmap for correlations between categorical variables\n",
    "plt.figure(figsize=[20,9])\n",
    "sb.heatmap(corr_matrix, annot=True,\n",
    "          vmin=-1, vmax=1, center=0,\n",
    "          cmap='YlGnBu')\n",
    "plt.title('Heatmap of correlations between categorical variables');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of less than 1 for correlations between a feature and itself can be explained by very small sample sizes for the chi-squared test that cramer's v builds on. Moreover, there are a few correlations between the weather type features which I should exploit and merge together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable to be used below to iterate through the columns and plot them\n",
    "season_names = ['season_spring', 'season_summer', 'season_fall']\n",
    "\n",
    "# plot boxplots for season versus number of users\n",
    "plt.figure(figsize = [15, 5])\n",
    "\n",
    "# boxplot for feature workingday\n",
    "plt.subplot(1, 3, 1)\n",
    "sb.boxplot(data = bike_df, x = 'season_spring', y = 'total_cust');\n",
    "\n",
    "# boxplot for feature weekday\n",
    "plt.subplot(1, 3, 2)\n",
    "sb.boxplot(data = bike_df, x = 'season_summer', y = 'total_cust');\n",
    "\n",
    "# boxplot for feature holiday\n",
    "plt.subplot(1, 3, 3)\n",
    "sb.boxplot(data = bike_df, x = 'season_fall', y = 'total_cust');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between season features and the maximum temperature\n",
    "# using the Kruskal Wallis H test for correlations between a continuous and categorical variable\n",
    "kruskal(bike_df['temp_max'], bike_df['season_summer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The season feature clearly determines the customer demand for bikes, so this feature will be used for the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### holiday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features holiday, weekday and workingday have some overlaps in their prediction of customer demand, thus, I will first analyze each individual feature and then investigate their correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the customer statistics in form of a boxplot for the holiday feature\n",
    "sb.boxplot(data = bike_df, x = 'holiday', y = 'total_cust');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the holiday feature, we can clearly see that there is on average a higher demand for bikes on days that are not holidays. This feature will be used in the final model to predict the overall demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between holiday feature and the number of customers per day\n",
    "# using the Kruskal Wallis H test for correlations between a continuous and categorical variable\n",
    "kruskal(bike_df['holiday'], bike_df['total_cust'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the customer statistics in form of a boxplot for the weekday feature\n",
    "plt.figure(figsize = [15, 5])\n",
    "sb.boxplot(data = bike_df, x = 'weekday', y = 'total_cust');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between weekday feature and the number of customers per day\n",
    "# using Pearson's correlation coefficient because I'm assuming that weekday can be \n",
    "# considered a continuous variable\n",
    "pearsonr(bike_df['weekday'], bike_df['total_cust'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above distributions of number of customers per weekday, it appears that there are slight difference in demand depending on what weekday it is. Thus, the weekday will be considered to forecast the bike demand. But I still need to onehot encode the weekday because it is a categorical feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### workingday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting workday feature in boxplot against the count of customers\n",
    "sb.boxplot(data = bike_df, x = 'working_day', y = 'total_cust');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between working_day feature and the number of customers per day\n",
    "# using the Kruskal Wallis H test for correlations between a continuous and categorical variable\n",
    "kruskal(bike_df['working_day'], bike_df['total_cust'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe that encodes the weekday feature with 0 for monday through friday\n",
    "# and 1 for saturday and sunday\n",
    "weekend_distinct_df = bike_df.copy()\n",
    "weekend_distinct_df['weekday'] = weekend_distinct_df['weekday'].apply(lambda x: 1 if (x == 6 or x == 0) else 0)\n",
    "\n",
    "# plot boxplots for comparison between the weekday and workingday feature\n",
    "plt.figure(figsize = [15, 5])\n",
    "\n",
    "# boxplot for feature workingday\n",
    "plt.subplot(1, 3, 1)\n",
    "sb.boxplot(data = bike_df, x = 'working_day', y = 'total_cust');\n",
    "\n",
    "# boxplot for feature weekday\n",
    "plt.subplot(1, 3, 2)\n",
    "sb.boxplot(data = weekend_distinct_df, x = 'weekday', y = 'total_cust');\n",
    "\n",
    "# boxplot for feature holiday\n",
    "plt.subplot(1, 3, 3)\n",
    "sb.boxplot(data = bike_df, x = 'holiday', y = 'total_cust');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the means of each instance of workingday\n",
    "bike_df.groupby('working_day')['total_cust'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the means of each instance of weekday\n",
    "weekend_distinct_df.groupby('weekday')['total_cust'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the means of each instance of holiday\n",
    "bike_df.groupby('holiday')['total_cust'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features holiday, weekday and workingday are correlated with each other as well as with the target variable total_cust. However, they contain slightly different information that may be useful for predicting the target variable. Thus, I will keep all three variables for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the weekday feature and only keeping dummy variables\n",
    "bike_df.drop(columns=['weekday'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot of target valuable depending on months\n",
    "plt.figure(figsize=[10,6])\n",
    "sb.boxplot(bike_df['month'], bike_df['total_cust'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### weather type features wt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wt_ features contain the following features and meanings:\n",
    "- wt_fog: fog, ice fog, or freezing fog (may include heavy fog)\n",
    "- wt_heavy_fog: heavy fog or heaving freezing fog (not always distinguished from fog)\n",
    "- wt_thunder: thunder\n",
    "- wt_sleet: ice pellets, sleet, snow pellets, or small hail\n",
    "- wt_hail: hail (may include small hail)\n",
    "- wt_glaze: glaze or rime\n",
    "- wt_haze: smoke or haze\n",
    "- wt_drift_snow: blowing or drifting snow \n",
    "- wt_high_wind: high or damaging winds\n",
    "- wt_mist: mist\n",
    "- wt_drizzle: drizzle\n",
    "- wt_rain: rain (may include freezing rain, drizzle, and freezing drizzle)\n",
    "- wt_freeze_rain: freezing rain\n",
    "- wt_snow: snow, snow pellets, snow grains, or ice crystals\n",
    "- wt_ground_fog: ground fog\n",
    "- wt_ice_fog: ice for or freezing fog\n",
    "- wt_freeze_drizzle: freezing drizzle\n",
    "- wt_unknown: unknown source of precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the revenue of the most common production companies vs. the rest\n",
    "fig, ax = plt.subplots(6, 3, figsize = [16, 25])\n",
    "\n",
    "# create list with all feature names \n",
    "wt_feat_list = [x for x in bike_df.columns if 'wt_' in x]\n",
    "\n",
    "# company counter\n",
    "counter = 0\n",
    "\n",
    "for j in range(len(ax)):\n",
    "    for i in range(len(ax[j])):\n",
    "        if j == 5 and i == 3:\n",
    "            break\n",
    "        else:\n",
    "            ax[j][i] = sb.boxplot(data = bike_df, x = wt_feat_list[counter], y = 'total_cust', ax=ax[j][i])\n",
    "            ax[j][i].set_ylabel('Number of customers')\n",
    "            ax[j][i].set_xlabel(wt_feat_list[counter])\n",
    "            counter += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the distributions of the target value depending on the weather feature as well as based on the assessment of similar descriptive weather names/patterns, I will merge some of the weather features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fog, heavy fog, hail, haze, high wind\n",
    "bike_df['foggy'] = bike_df['wt_fog'] + bike_df['wt_heavy_fog'] + bike_df['wt_hail'] + bike_df['wt_haze'] + bike_df['wt_high_wind']\n",
    "bike_df['foggy'] = bike_df['foggy'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# thunder\n",
    "bike_df['thunder'] = bike_df['wt_thunder']\n",
    "\n",
    "# ice_fog, unknown, freeze_drizzle, freeze_rain, drift_snow\n",
    "bike_df['ice'] = bike_df['wt_ice_fog'] + bike_df['wt_unknown'] + bike_df['wt_freeze_drizzle'] + bike_df['wt_freeze_rain'] + bike_df['wt_drift_snow']\n",
    "bike_df['ice'] = bike_df['ice'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# sleet, glaze, snow\n",
    "bike_df['sleet'] = bike_df['wt_sleet'] + bike_df['wt_glaze'] + bike_df['wt_snow']\n",
    "bike_df['sleet'] = bike_df['sleet'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# mist, drizzle, rain, ground fog\n",
    "bike_df['rain'] = bike_df['wt_mist'] + bike_df['wt_drizzle'] + bike_df['wt_rain'] + bike_df['wt_ground_fog']\n",
    "bike_df['rain'] = bike_df['rain'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the old wt features\n",
    "bike_df.drop(columns=wt_feats, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the acf of wt_ features\n",
    "plt.figure(figsize=[15,6])\n",
    "\n",
    "plot_acf(bike_df['foggy'], title='PACF: All samples in metro DC area',)\n",
    "plt.show()\n",
    "\n",
    "plot_acf(bike_df['thunder'], title='PACF: All samples in metro DC area',)\n",
    "plt.show()\n",
    "\n",
    "plot_acf(bike_df['ice'], title='PACF: All samples in metro DC area',)\n",
    "plt.show()\n",
    "\n",
    "plot_acf(bike_df['sleet'], title='PACF: All samples in metro DC area',)\n",
    "plt.show()\n",
    "\n",
    "plot_acf(bike_df['rain'], title='PACF: All samples in metro DC area',)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting all newly created features\n",
    "plt.figure(figsize=[20,9])\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "plt.plot(bike_df['thunder'])\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(bike_df['foggy'])\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(bike_df['ice'])\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(bike_df['sleet'])\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(bike_df['rain'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly created rain feature seems to have been discontinued after two years into the time series. I will therefore drop this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df.drop(columns=['rain'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list for all correlations between temp_max and total_cust with different rolling means\n",
    "\n",
    "def best_window_sum(x, y, max_window):\n",
    "    corr_temp_cust = []\n",
    "    for i in range(1, max_window):\n",
    "        roll_val = list(x.rolling(i).sum()[i-1:-1])\n",
    "        total_cust_ti = list(y[i:])\n",
    "        corr, p_val = pearsonr(total_cust_ti, roll_val)\n",
    "        corr_temp_cust.append(corr)\n",
    "    # get the optimal window size for rolling mean between temp_max and total_cust\n",
    "    max_val = np.argmax(corr_temp_cust)\n",
    "    min_val = np.argmin(corr_temp_cust)\n",
    "    opt_corr_min = corr_temp_cust[min_val]\n",
    "    opt_corr_max = corr_temp_cust[max_val]\n",
    "    \n",
    "    results = {max_val+1: opt_corr_max, min_val+1: opt_corr_min}\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal window for rolling std for temperature\n",
    "print(best_window_sum(bike_df['foggy'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by temp_max\n",
    "foggy_mean = bike_df['foggy'].rolling(16).sum()[15:-1]\n",
    "pearsonr(foggy_mean, bike_df['total_cust'][16:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal window for rolling std for temperature\n",
    "print(best_window_sum(bike_df['thunder'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by temp_max\n",
    "thunder_mean = bike_df['thunder'].rolling(16).sum()[15:-1]\n",
    "pearsonr(thunder_mean, bike_df['total_cust'][16:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal window for rolling std for temperature\n",
    "print(best_window_sum(bike_df['ice'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by temp_max\n",
    "ice_mean = bike_df['ice'].rolling(16).sum()[15:-1]\n",
    "pearsonr(ice_mean, bike_df['total_cust'][16:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal window for rolling std for temperature\n",
    "print(best_window_sum(bike_df['sleet'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by temp_max\n",
    "sleet_mean = bike_df['sleet'].rolling(16).sum()[15:-1]\n",
    "pearsonr(sleet_mean, bike_df['total_cust'][16:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all distributions and scatterplot between each continuous variable pair\n",
    "sb.pairplot(bike_df, vars=['temp_min', 'temp_max', 'temp_observ', 'wind', 'precip', 'total_cust']);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the above pairplot, the following things are apparent:\n",
    "* **wind looks like a Weibull distribution**\n",
    "* there are almost **perfect linear relationships among the four temp features**\n",
    "* **precip feature is left skewed**\n",
    "* there is **no linear relationship between precip and any other feature**\n",
    "* **wind** has **no linear relationship with any other feature**\n",
    "* the **temp features** have a **medium strong linear relationship with the total_cust target label*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation matrix\n",
    "bike_df_corr = bike_df[['temp_min', 'temp_max', 'temp_observ', 'wind', 'precip', 'total_cust']].corr()\n",
    "\n",
    "# create a heatmap to visualize the results\n",
    "plt.figure(figsize=[10,6])\n",
    "sb.heatmap(bike_df_corr, annot=True,\n",
    "          vmin=-1, vmax=1, center=0,\n",
    "          cmap='YlGnBu')\n",
    "plt.title('Heatmap of correlations between continuous variables');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap underlines the indications of the pairplot and a number of steps need to be taken:\n",
    "* the temp features are highly correlated with each other. I will keep both temp_max and temp_min because I tried it in the model and it was working better with keeping both features and Zeng et al. 2012 also use both temp_min and temp_max. \n",
    "* there are only low to very low negative correlations between wind and precip with the target label, respectively. However, I will keep both features in my model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list for all correlations between temp_max and total_cust with different rolling means\n",
    "\n",
    "def best_window(x, y, max_window):\n",
    "    corr_temp_cust = []\n",
    "    for i in range(1, max_window):\n",
    "        roll_val = list(x.rolling(i).mean()[i-1:-1])\n",
    "        total_cust_ti = list(y[i:])\n",
    "        corr, p_val = pearsonr(total_cust_ti, roll_val)\n",
    "        corr_temp_cust.append(corr)\n",
    "    # get the optimal window size for rolling mean between temp_max and total_cust\n",
    "    max_val = np.argmax(corr_temp_cust)\n",
    "    min_val = np.argmin(corr_temp_cust)\n",
    "    opt_corr_min = corr_temp_cust[min_val]\n",
    "    opt_corr_max = corr_temp_cust[max_val]\n",
    "    \n",
    "    results = {max_val+1: opt_corr_max, min_val+1: opt_corr_min}\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list for all correlations between temp_max and total_cust with different rolling standard deviations\n",
    "\n",
    "def best_window_std(x, y, max_window):\n",
    "    corr_temp_cust = []\n",
    "    for i in range(2, max_window):\n",
    "        roll_val = list(x.rolling(i).std()[i-1:-1])\n",
    "        total_cust_ti = list(y[i:])\n",
    "        corr, p_val = pearsonr(total_cust_ti, roll_val)\n",
    "        corr_temp_cust.append(corr)\n",
    "    # get the optimal window size for rolling mean between temp_max and total_cust\n",
    "    max_val = np.argmax(corr_temp_cust)\n",
    "    min_val = np.argmin(corr_temp_cust)\n",
    "    opt_corr_min = corr_temp_cust[min_val]\n",
    "    opt_corr_max = corr_temp_cust[max_val]\n",
    "    \n",
    "    results = {max_val+1: opt_corr_max, min_val+1: opt_corr_min}\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### total_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the overall total_cust values for entire timeseries\n",
    "plt.figure(figsize=[15,6])\n",
    "ax = sb.lineplot(x='date_datetime', y='total_cust', data=bike_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very obvious that this time series is non-stationary. I need to deal with this later on before using the data in my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot only last two years of timeseries\n",
    "plt.figure(figsize=[15,6])\n",
    "ax = sb.lineplot(x='date_datetime', y='total_cust', data=bike_df[-718:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PACF to determine optimal lag for total_cust target label**\n",
    "\n",
    "Details on the process can be found [here](https://towardsdatascience.com/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8) and [here](https://towardsdatascience.com/understanding-partial-auto-correlation-fa39271146ac). We want to avoid using variables with multicolinearity and thus, we do not want to use too many lag variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(bike_df['total_cust'], title='PACF: All samples in metro DC area',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above plot, a lag of 1 shows significant correlation with t+0 and therefore. This analysis is mainly important for the baseline model that I will be implementing which is a simple autoregression model. The other models I will implement need to be evaluated against that. This is not relevant for the rest of the analysis, but I will keep it in this notebook for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal window for rolling std for temperature\n",
    "print(best_window_std(bike_df['total_cust'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by total_cust\n",
    "cust_mean = bike_df['total_cust'].rolling(16).std()[15:-1]\n",
    "pearsonr(cust_mean, bike_df['total_cust'][16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal number for rolling mean window\n",
    "print(best_window(bike_df['total_cust'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by total_cust\n",
    "cust_mean = bike_df['total_cust'].rolling(16).mean()[15:-1]\n",
    "pearsonr(cust_mean, bike_df['total_cust'][16:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the value from t-1\n",
    "bike_df['total_cust_t-1'] = bike_df['total_cust'].shift()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### temp_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series that group the mean temperature per season\n",
    "temp_spring = bike_df.groupby('season_spring')['temp_max'].mean().rename({1: 'Spring'})\n",
    "temp_summer = bike_df.groupby('season_summer')['temp_max'].mean().rename({1: 'Summer'})\n",
    "temp_fall = bike_df.groupby('season_fall')['temp_max'].mean().rename({1: 'Fall'})\n",
    "\n",
    "# add them to one series and drop the rows with index 0\n",
    "temp_seasons = temp_spring.append(temp_summer).append(temp_fall)\n",
    "temp_seasons.drop(labels=[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average temp_max per season\n",
    "plt.figure(figsize=[10,6])\n",
    "sb.barplot(x=temp_seasons.index, y=temp_seasons.values);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series that groups average users per season\n",
    "cust_spring = bike_df.groupby('season_spring')['total_cust'].mean().rename({1: 'Spring'})\n",
    "cust_summer = bike_df.groupby('season_summer')['total_cust'].mean().rename({1: 'Summer'})\n",
    "cust_fall = bike_df.groupby('season_fall')['total_cust'].mean().rename({1: 'Fall'})\n",
    "\n",
    "# add them to one series and drop the rows with index 0\n",
    "cust_seasons = cust_spring.append(cust_summer).append(cust_fall)\n",
    "cust_seasons.drop(labels=[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign x and y1 and y2\n",
    "x = list(temp_seasons.index)\n",
    "y1 = cust_seasons.values\n",
    "y2 = temp_seasons.values\n",
    "\n",
    "# below code adapted from https://matplotlib.org/gallery/api/two_scales.html\n",
    "# creat plot containing both average count of customers\n",
    "# and average temp per month\n",
    "plt.figure(figsize=[15,7])\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color1 = 'tab:red'\n",
    "ax1.set_xlabel('Seasons')\n",
    "ax1.set_ylabel('Average number of customers', color=color1)\n",
    "ax1.bar(x, y1, color=color1)\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "color2 = 'tab:blue'\n",
    "ax2.set_ylabel('Average maximum temperature', color=color2)\n",
    "ax2.plot(x, y2, color=color2)\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph we can see that the month variable is correlated with the average number of customers per that month as well as the average temperature. The temperature strongly determines the number of customers. The temperature is likely a very important feature for predicting the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting temp feature against the target label cnt\n",
    "plt.figure(figsize=[7,5])\n",
    "\n",
    "sb.scatterplot(data = bike_df, x = 'temp_max', y = 'total_cust', color='green')\n",
    "plt.xlabel('Maximum temperature')\n",
    "plt.ylabel('Number of customers')\n",
    "plt.title('Maximum temperature vs. number of customers');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the partial autocorrelation of temp_max\n",
    "plot_pacf(bike_df['temp_max'], title='PACF: All samples in metro DC area',)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autocorrelation of temp_max\n",
    "plot_acf(bike_df['temp_max'], title='PACF: All samples in metro DC area',)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal window for rolling std for temperature\n",
    "print(best_window_std(bike_df['temp_max'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by temp_max\n",
    "temp_mean = bike_df['temp_max'].rolling(16).std()[15:-1]\n",
    "pearsonr(temp_mean, bike_df['total_cust'][16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal window for rolling mean for temperature\n",
    "best_window(bike_df['temp_max'], bike_df['total_cust'], 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the overall temp_max values for entire timeseries\n",
    "plt.figure(figsize=[15,6])\n",
    "ax = sb.lineplot(x='date_datetime', y='temp_max', data=bike_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot of rolling means\n",
    "plt.figure(figsize=[15,6])\n",
    "\n",
    "plt.plot(bike_df['temp_max'].rolling(16).mean());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot of rolling stds\n",
    "plt.figure(figsize=[15,6])\n",
    "\n",
    "plt.plot(bike_df['temp_max'].rolling(16).std());\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### temp_min\n",
    "I will continue to use temp_min for my analysis as well, even though there is a high correlation between the temp_max and temp_min feature. Collinearity does not matter with tree-based methods, therefore, I can keep it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting temp feature against the target label cnt\n",
    "plt.figure(figsize=[7,5])\n",
    "\n",
    "sb.scatterplot(data = bike_df, x = 'temp_min', y = 'total_cust', color='red')\n",
    "plt.xlabel('Minimum temperature')\n",
    "plt.ylabel('Number of customers')\n",
    "plt.title('Minimum temperature vs. number of customers');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will categorize the temperature similarly to what the paper by El-Assi et al. (2015) did:\n",
    "* Very Cold (below 0) \n",
    "* Cold (between 0 and 10)\n",
    "* Cool (between 10 and 20) \n",
    "* Warm (between 20 to 30)\n",
    "* Hot (30 or more)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineer new categorical temp features\n",
    "bike_df_cat = bike_df.copy()\n",
    "bike_df_cat['very_cold'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 0 else 0)\n",
    "bike_df_cat['cold'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 10 and x >= 0 else 0)\n",
    "bike_df_cat['cool'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 20 and x >= 10 else 0)\n",
    "bike_df_cat['warm'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 30 and x >= 20 else 0)\n",
    "bike_df_cat['hot'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x >= 30 else 0)\n",
    "\n",
    "bike_df_cat[['very_cold', 'cold', 'cool', 'warm', 'hot']] = bike_df_cat[['very_cold', 'cold', 'cool', 'warm', 'hot']].shift()\n",
    "\n",
    "\n",
    "bike_df_cat = bike_df_cat.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(bike_df_cat['total_cust'], bike_df_cat['cold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot categorized temperature\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.subplot(2,3,1)\n",
    "sb.boxplot(bike_df_cat['very_cold'], bike_df_cat['total_cust'])\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "sb.boxplot(bike_df_cat['cold'], bike_df_cat['total_cust'])\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "sb.boxplot(bike_df_cat['cool'], bike_df_cat['total_cust'])\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "sb.boxplot(bike_df_cat['warm'], bike_df_cat['total_cust'])\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "sb.boxplot(bike_df_cat['hot'], bike_df_cat['total_cust'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the distribution of precip\n",
    "plt.figure(figsize=[15,6])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(bike_df['precip'], bins=15)\n",
    "plt.title('Distribution of precip')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sb.scatterplot(data = bike_df, x = 'precip', y = 'total_cust')\n",
    "plt.xlabel('Precipitation')\n",
    "plt.ylabel('Number of customers')\n",
    "plt.title('Precipitation vs. number of customers');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although precip is only correlated with total_cust in a weak sense, I will keep this in the model. This distribution is also left-skewed, so a logarithmic transformation will be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the distribution of precip\n",
    "plt.figure(figsize=[10,6])\n",
    "\n",
    "x = np.log10(bike_df['precip'] + 1)\n",
    "plt.hist(x)\n",
    "plt.title('Distribution of precip');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the overall precip values for entire timeseries\n",
    "plt.figure(figsize=[15,6])\n",
    "ax = sb.lineplot(x='date_datetime', y='precip', data=bike_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this timeseries, I will also use a test to determine whether this timeseries is non-stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal number for rolling mean window\n",
    "print(best_window(bike_df['precip'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by temp_max\n",
    "precip_mean = bike_df['precip'].rolling(16).mean()[15:-1]\n",
    "pearsonr(precip_mean, bike_df['total_cust'][16:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal window for rolling std for temperature\n",
    "print(best_window_std(bike_df['precip'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by temp_max\n",
    "precip_mean = bike_df['precip'].rolling(16).std()[15:-1]\n",
    "pearsonr(precip_mean, bike_df['total_cust'][16:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the partial autocorrelation of precip\n",
    "plot_pacf(bike_df['precip'], title='PACF: All samples in metro DC area',)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot of rolling means\n",
    "plt.figure(figsize=[15,6])\n",
    "\n",
    "plt.plot(bike_df['precip'].rolling(16).mean());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot of rolling stds\n",
    "plt.figure(figsize=[15,6])\n",
    "\n",
    "plt.plot(bike_df['precip'].rolling(16).std());\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the distribution of wind\n",
    "plt.figure(figsize=[15,6])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(bike_df['wind'], bins=15)\n",
    "plt.title('Distribution of wind')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sb.scatterplot(data = bike_df, x = 'wind', y = 'total_cust')\n",
    "plt.xlabel('Wind')\n",
    "plt.ylabel('Number of customers')\n",
    "plt.title('Wind vs. number of customers');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the overall wind values for entire timeseries\n",
    "plt.figure(figsize=[15,6])\n",
    "ax = sb.lineplot(x='date_datetime', y='wind', data=bike_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although windspeed is only correlated with cnt in a weak sense, similar to precip, I will keep this in the model. To determine stationarity, I will use a statistical test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal number for rolling mean window\n",
    "print(best_window(bike_df['wind'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by temp_max\n",
    "wind_mean = bike_df['wind'].rolling(16).mean()[15:-1]\n",
    "pearsonr(wind_mean, bike_df['total_cust'][16:])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the optimal window for rolling std for temperature\n",
    "print(best_window_std(bike_df['wind'], bike_df['total_cust'], 30))\n",
    "\n",
    "# get the correlation for window size determined by temp_max\n",
    "precip_mean = bike_df['wind'].rolling(21).std()[20:-1]\n",
    "pearsonr(precip_mean, bike_df['total_cust'][21:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the partial autocorrelation of wind\n",
    "plot_pacf(bike_df['wind'], title='PACF: All samples in metro DC area',)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I am not forecasting wind, but rather trying to understand whether wind could be able to forecast total_cust, the partial autocorrelation is not of much interest to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot of rolling means\n",
    "plt.figure(figsize=[15,6])\n",
    "\n",
    "plt.plot(bike_df['wind'].rolling(16).mean());\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot of rolling means\n",
    "plt.figure(figsize=[15,6])\n",
    "\n",
    "plt.plot(bike_df['wind'].rolling(16).std());\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Checking and dealing with stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very important part before prediction can be accurate and successful is to make the time series stationary. Examples of how to do this can be found [here](https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/) for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Augmented Dickey Fuller Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code based on implementation on https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/\n",
    "def adf_test(df, col_names):\n",
    "    '''\n",
    "    Function to perform Augmented Dickey-Fuller test on selected timeseries\n",
    "    Args: df = dataframe with timeseries to be tested\n",
    "          col_names = list of names of the timeseries to be tested\n",
    "    Returns: None\n",
    "    '''\n",
    "    for name in col_names:\n",
    "        print ('Results of Augmented Dickey-Fuller Test for {}'.format(name))\n",
    "        result_test = adfuller(df[name], autolag='AIC')\n",
    "        result_output = pd.Series(result_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "        for key, val in result_test[4].items():\n",
    "            result_output['Critical Value (%s)'%key] = val\n",
    "        print (result_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the features that need to be tested\n",
    "# total_cust_t-1 was already added to the dataframe\n",
    "\n",
    "testing_feat = ['wind', 'precip', 'total_cust', 'temp_min', 'temp_max', 'foggy', 'ice', 'thunder', 'sleet']\n",
    "\n",
    "testing_df = pd.DataFrame()\n",
    "\n",
    "for col in testing_feat:\n",
    "    col_mean = bike_df[col].rolling(16).mean()[15:-1]\n",
    "    col_std = bike_df[col].rolling(16).std()[15:-1]\n",
    "    testing_df[col+'_mean16'] = col_mean.values\n",
    "    testing_df[col+'_std16'] = col_std.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adf test for total_cust_t-1\n",
    "temp_cust_1 = bike_df['total_cust_t-1'].fillna(0)\n",
    "bike_df_temp = pd.DataFrame(temp_cust_1, columns=['total_cust_t-1'])\n",
    "adf_test(bike_df_temp, ['total_cust_t-1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adf test for total_cust\n",
    "adf_test(bike_df, ['total_cust'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adf test for all engineered features\n",
    "adf_test(testing_df, testing_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null hypothesis: data is not stationary\n",
    "\n",
    "Alternative hypothesis: data is stationary\n",
    "\n",
    "The results of the ADF Test can be intepreted as follows:\n",
    "* test-statistic < critical value --> reject null hypothesis (data is stationary)\n",
    "* test_statistic > critical value --> fail to reject null hypothesis (data is not stationary)\n",
    "\n",
    "Based on these results, total_cust, total_cust_t-1 and total_cust_mean16 are not stationary at the 1%-level. This test does not look at the trend stationarity but rather at the difference stationarity. This means that total_cust_t-1 and total_cust_mean16 are **not difference stationary** at the 1%-level. The KPSS test is necessary to detemine the trend stationarity of the timeseries.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kwiatkowski-Phillips-Schmidt-Shin Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code based on implementation on https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/\n",
    "def kpss_test(df, col_names):\n",
    "    '''\n",
    "    Function to perform KPSS test on selected timeseries\n",
    "    Args: df = dataframe with timeseries to be tested\n",
    "          col_names = list of names of the timeseries to be tested\n",
    "    Returns: None\n",
    "    '''\n",
    "    for name in col_names:\n",
    "        print ('Results of KPSS Test for {}'.format(name))\n",
    "        result_test = kpss(df[name], regression='c', lags='legacy')\n",
    "        result_output = pd.Series(result_test[0:3], index=['Test Statistic','p-value','Lags Used'])\n",
    "        for key, val in result_test[3].items():\n",
    "            result_output['Critical Value (%s)'%key] = val\n",
    "        print (result_output)\n",
    "\n",
    "# kpss test for total_cust\n",
    "kpss_test(testing_df, testing_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kpss test for total_cust_t-1\n",
    "kpss_test(bike_df_temp, ['total_cust_t-1'])\n",
    "\n",
    "# kpss test for total_cust\n",
    "kpss_test(bike_df, ['total_cust'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null hypothesis: data is stationary\n",
    "\n",
    "Alternative hypothesis: data is not stationary\n",
    "\n",
    "The results of the KPSS Test can be intepreted as follows:\n",
    "* test-statistic > critical value --> reject null hypothesis (data is not stationary)\n",
    "* test_statistic < critical value --> fail to reject null hypothesis (data is stationary)\n",
    "\n",
    "Based on these results, all total_cust and rain features are not trend stationary at the 1%-level.\n",
    "\n",
    "This has the following implications for the evaluated timeseries:\n",
    "* difference and trend stationary: **no** transformations are needed\n",
    "* difference stationary and **not** trend stationary: total_cust_std16 --> transformations needed\n",
    "* **not** difference stationary but trend stationary: total_cust --> transformations needed\n",
    "* **not** difference and **not** trend stationary: total_cust_mean16, total_cust_t-1 --> transformations needed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trend\n",
    "For trend, we're using log transformations: https://medium.com/@stallonejacob/time-series-forecast-a-basic-introduction-using-python-414fcb963000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing positive upward trend from total_cust_mean16, total_cust_std16 and total_cust_t-1\n",
    "testing_df['total_cust_std16_log'] = [np.log1p(x+1) for x in testing_df['total_cust_std16']]\n",
    "testing_df['total_cust_mean16_log'] = [np.log1p(x+1) for x in testing_df['total_cust_mean16']]\n",
    "bike_df_temp['total_cust_t-1_log'] = [np.log1p(x+1) for x in bike_df_temp['total_cust_t-1']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying differencing to remove trend from total_cust\n",
    "bike_df_check = bike_df[['total_cust']]\n",
    "bike_df_check['total_cust_diff'] = bike_df_check['total_cust'] - bike_df_check['total_cust'].shift()\n",
    "bike_df_check = bike_df_check.iloc[1:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kpss test for total_cust\n",
    "kpss_test(testing_df, ['total_cust_std16_log', 'total_cust_mean16_log'])\n",
    "kpss_test(bike_df_temp, ['total_cust_t-1_log'])\n",
    "\n",
    "# adf test for total_cust\n",
    "adf_test(testing_df, ['total_cust_std16_log', 'total_cust_mean16_log'])\n",
    "adf_test(bike_df_temp, ['total_cust_t-1_log'])\n",
    "adf_test(bike_df_check, ['total_cust_diff'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations I implemented above can be used later on to make the timeseries stationary. I will create classes to make the necessary transformations inside the ML pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Feature Selection\n",
    "\n",
    "Based on the EDA above, I will choose the following features\n",
    "* holiday: t0\n",
    "* weekdays: each day at t0\n",
    "* workingday: t0\n",
    "* temp_max: rolling mean for 16 days\n",
    "* temp_max: rolling std for 16 days\n",
    "* season_spring: t0\n",
    "* season_summer: t0\n",
    "* season_fall: t0\n",
    "* wt_ features: sum for 16 days\n",
    "* wind: rolling mean for 16 days\n",
    "* wind: rolling std for 16 days\n",
    "* precip: rolling mean for 16 days\n",
    "* precip: rolling std for 16 days\n",
    "* total_cust: t-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preparation\n",
    "\n",
    "#### 3.1. Dropping unnecessary columns\n",
    "\n",
    "I am only going to remove any columns which I no longer need and split the dataframes into X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should keep this before the cleaning function has been created\n",
    "bike_df.drop(columns=['temp_observ', 'month'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the timestamp variable\n",
    "bike_df.drop(columns=['date_datetime'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the window for rolling values\n",
    "window = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineer new categorical temp features\n",
    "bike_df_cat = bike_df[['temp_max']].copy()\n",
    "bike_df_cat['very_cold'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 0 else 0)\n",
    "bike_df_cat['cold'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 10 and x >= 0 else 0)\n",
    "bike_df_cat['cool'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 20 and x >= 10 else 0)\n",
    "bike_df_cat['warm'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x < 30 and x >= 20 else 0)\n",
    "bike_df_cat['hot'] = bike_df_cat['temp_max'].apply(lambda x: 1 if x >= 30 else 0)\n",
    "\n",
    "bike_df_cat[['very_cold', 'cold', 'cool', 'warm', 'hot']] = bike_df_cat[['very_cold', 'cold', 'cool', 'warm', 'hot']].shift()\n",
    "\n",
    "bike_df_cat.drop(columns=['temp_max'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating rolling values\n",
    "new_feat = ['wind', 'precip', 'total_cust', 'temp_max', 'temp_min', 'foggy', 'ice', 'thunder', 'sleet']\n",
    "\n",
    "temp_df = pd.DataFrame()\n",
    "\n",
    "for col in new_feat:\n",
    "    col_mean = bike_df[col].rolling(window).mean()[(window-1):-1]\n",
    "    col_std = bike_df[col].rolling(window).std()[(window-1):-1]\n",
    "    temp_df[col+'_mean'+str(window)] = col_mean.values\n",
    "    temp_df[col+'_std'+str(window)] = col_std.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remember to remove the first row from 16 rows from total_cust (target label)\n",
    "new_bike_df = bike_df.iloc[window:,:]\n",
    "bike_df_cat = bike_df_cat.iloc[window:,:]\n",
    "new_bike_df.reset_index(drop=True, inplace=True)\n",
    "bike_df_cat.reset_index(drop=True, inplace=True)\n",
    "new_bike_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns which we cannot use because of lookahead bias\n",
    "new_bike_df.drop(columns=['temp_max', 'wind', 'temp_min', 'precip', \n",
    "                          'thunder', 'foggy', 'ice', 'sleet'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merging both dataframes with features\n",
    "final_bike_df = new_bike_df.join(temp_df, how='left')\n",
    "final_bike_df = final_bike_df.join(bike_df_cat, how='left')\n",
    "final_bike_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Assinging X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning X and y\n",
    "y_norm = final_bike_df['total_cust']\n",
    "y = (y_norm - y_norm.shift())[1:]\n",
    "X_norm = final_bike_df.drop(columns=['total_cust'])\n",
    "X = X_norm[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful --> this is time series data so I need to be careful when splitting the dataset\n",
    "\n",
    "I may not need to split it when using pipelines, but I need to research how to do this\n",
    "\n",
    "My pipeline should include ALL transformations, including onehot encoding, that I make after the very initial cleaning of data. Thus, I need to read in my data again after the EDA process.\n",
    "\n",
    "If I'm using gradient descent I most DEFINITELY need to convert month and year to categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my model needs to use the cnt of today and check how the weather of the previous week predicted cnt\n",
    "\n",
    "or I could use the forecast of today and see how the forecast predicted the cnt\n",
    "\n",
    "day 1 day 2 day 3 day 4 day 5 day 6 day 7 --> predict cnt of day 8\n",
    "\n",
    "look at the mean and standard deviation of the \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Creating transformation classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a class that I can use in the ML pipeline that prints out the transformed data that will enter the model\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.shape = X.shape\n",
    "        print(self.shape)\n",
    "        X_df = pd.DataFrame(X)\n",
    "        print(X_df)\n",
    "        # print(X_df.to_string()) # can only be print like this without running LagVars() to avoid crashing\n",
    "        # what other output you want\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a naive estimator that can be used in a pipeline\n",
    "# for more details check here: https://machinelearningmastery.com/how-to-grid-search-naive-methods-for-univariate-time-series-forecasting/\n",
    "class NaiveEstimator(BaseEstimator, RegressorMixin):  \n",
    "    '''\n",
    "    This estimator does the following: if today is Monday, it looks at yesterday, Sunday,\n",
    "    to forecast the value of today; thus, there is a 1 day lag between the forecast\n",
    "    and the lookup value; however, this can be adapted by changing the input of window\n",
    "    and the input of t_future\n",
    "    '''\n",
    "    def __init__(self, window=1, t_future=0):\n",
    "        self.window = window\n",
    "        self.t_future = t_future\n",
    "        self.total_span = window+t_future\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        self.y_pred = []\n",
    "        missing_vals = [0 for x in range(0, self.total_span)]\n",
    "        self.y_pred.extend(missing_vals)\n",
    "        \n",
    "        for item in range(0, len(X)-self.total_span):\n",
    "\n",
    "            naive = X.iloc[item]\n",
    "            self.y_pred.append(naive)\n",
    "                \n",
    "        return self.y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Baseline model: naive univariate prediction\n",
    "\n",
    "When using autoregression models, we're only focusing on the target variable and aim to predict this variable with it previous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning X and y for the univariate naive prediction\n",
    "y_naive = final_bike_df['total_cust'].copy()\n",
    "X_naive = y_naive.copy()\n",
    "X_naive = pd.DataFrame(data=X_naive, columns=['total_cust'])\n",
    "#X_naive = (X_naive - X_naive.shift())[1:]\n",
    "#y_naive = (y_naive - y_naive.shift())[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for univariate naive prediction\n",
    "pipeline_naive = Pipeline([\n",
    "    ('model', NaiveEstimator())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a timeseries split of the datasets for naive prediction\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# doing cross validation on the chunks of data and calculating scores for naive prediction\n",
    "scores_naive = cross_validate(pipeline_naive, X_naive, y_naive, cv=time_split,\n",
    "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "                         return_train_score=True, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with naive prediction scores\n",
    "print('Naive model: Average root mean squared error train data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_naive['train_neg_mean_squared_error']])/len(scores_naive['train_neg_mean_squared_error']))\n",
    "print('Naive model: Average root mean squared error test data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_naive['test_neg_mean_squared_error']])/len(scores_naive['test_neg_mean_squared_error']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared log error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([(-1 * x) for x in scores_naive['train_neg_mean_absolute_error']])/len(scores_naive['train_neg_mean_absolute_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([(-1 * x) for x in scores_naive['test_neg_mean_absolute_error']])/len(scores_naive['test_neg_mean_absolute_error']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the model which is a Random Forest model and uses default hyperparameters\n",
    "model_rf = RandomForestRegressor(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and fitting the ML pipeline\n",
    "trend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
    "    ('model', model_rf)\n",
    "])\n",
    "\n",
    "pipeline_rf = pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a timeseries split of the datasets\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# doing cross validation on the chunks of data and calculating scores\n",
    "scores_rf = cross_validate(pipeline_rf, X, y, cv=time_split,\n",
    "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "                         return_train_score=True, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_rf['train_neg_mean_squared_error']])/len(scores_rf['train_neg_mean_squared_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_rf['test_neg_mean_squared_error']])/len(scores_rf['test_neg_mean_squared_error']))\n",
    "\n",
    "# root mean squared log error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([x for x in scores_rf['train_neg_mean_absolute_error']])/len(scores_rf['train_neg_mean_absolute_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([x for x in scores_rf['test_neg_mean_absolute_error']])/len(scores_rf['test_neg_mean_absolute_error']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use randomizedsearch and gridsearch to tune my hyperparameters for the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trees in random forest\n",
    "n_estimators = randint(50, 2000)\n",
    "# maximum number of features included in the model\n",
    "max_features = randint(5, 20)\n",
    "# maximum number of levels in tree\n",
    "max_depth = randint(1,7)\n",
    "# minimum number of samples required to split a node\n",
    "min_samples_split = randint(3, 10)\n",
    "\n",
    "# create the random grid\n",
    "random_grid_rf = {'model__n_estimators': n_estimators,\n",
    "                   'model__max_depth': max_depth,\n",
    "                   'model__min_samples_split': min_samples_split,\n",
    "                   'model__max_features': max_features}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the start time\n",
    "start_time = datetime.now()\n",
    "print(start_time)\n",
    "\n",
    "# instantiate and fit the randomizedsearch class to the random parameters\n",
    "rs_rf = RandomizedSearchCV(pipeline_rf, \n",
    "                        param_distributions=random_grid_rf, \n",
    "                        scoring='neg_mean_squared_error', \n",
    "                        n_jobs=-1,\n",
    "                        cv=time_split,\n",
    "                        n_iter = 5,\n",
    "                        verbose=10,\n",
    "                        random_state=23)\n",
    "rs_rf = rs_rf.fit(X, y)\n",
    "\n",
    "# print the total running time\n",
    "end_time = datetime.now()\n",
    "print('Total running time:', end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best parameters from randomizedsearch\n",
    "rs_rf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the best RandomForest model\n",
    "#pickle.dump(gs_rf.best_estimator_, open('randomforest.sav', 'wb'))\n",
    "\n",
    "# change the keys of the best_params dictionary to allow it to be used for the final model\n",
    "fix_best_params_rf = {key[7:]: val for key, val in rs_rf.best_params_.items()}\n",
    "\n",
    "print(fix_best_params_rf)\n",
    "\n",
    "# fit the randomforestregressor with the best_params as hyperparameters\n",
    "model_rf = RandomForestRegressor(**fix_best_params_rf)\n",
    "\n",
    "# fitting x and y to pipeline\n",
    "pipeline_rf_rs = pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing cross validation on the chunks of data and calculating scores\n",
    "scores_rf = cross_validate(pipeline_rf_rs, X, y, cv=time_split,\n",
    "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "                         return_train_score=True, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_rf['train_neg_mean_squared_error']])/len(scores_rf['train_neg_mean_squared_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_rf['test_neg_mean_squared_error']])/len(scores_rf['test_neg_mean_squared_error']))\n",
    "\n",
    "# root mean squared log error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([(-1 * x) for x in scores_rf['train_neg_mean_absolute_error']])/len(scores_rf['train_neg_mean_absolute_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([(-1 * x) for x in scores_rf['test_neg_mean_absolute_error']])/len(scores_rf['test_neg_mean_absolute_error']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign lists of parameters to be used in gridsearch\n",
    "param_grid_rf = {'model__max_depth': [4, 5],\n",
    "                 'model__max_features': [19, 20],\n",
    "                 'model__min_samples_split': [7, 8],\n",
    "                 'model__n_estimators': [90, 100]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gridsearch to search around the randomizedsearch best parameters and further improve the model\n",
    "gs_rf = GridSearchCV(pipeline_rf, \n",
    "                  param_grid=param_grid_rf, \n",
    "                  scoring='neg_mean_squared_error', \n",
    "                  verbose = 10,\n",
    "                  n_jobs=-1, \n",
    "                  cv=time_split)\n",
    "gs_rf = gs_rf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the best RandomForest model\n",
    "pickle.dump(gs_rf.best_estimator_, open('randomforest.sav', 'wb'))\n",
    "\n",
    "# change the keys of the best_params dictionary to allow it to be used for the final model\n",
    "fix_best_params_rf = {key[7:]: val for key, val in gs_rf.best_params_.items()}\n",
    "\n",
    "print(fix_best_params_rf)\n",
    "\n",
    "# fit the randomforestregressor with the best_params as hyperparameters\n",
    "model_rf = RandomForestRegressor(**fix_best_params_rf)\n",
    "\n",
    "# fitting x and y to pipeline\n",
    "pipeline_rf_tuned = pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing cross validation on the chunks of data and calculating scores\n",
    "scores_rf_tuned = cross_validate(pipeline_rf_tuned, X, y, cv=time_split,\n",
    "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "                         return_train_score=True, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_rf_tuned['train_neg_mean_squared_error']])/len(scores_rf_tuned['train_neg_mean_squared_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_rf_tuned['test_neg_mean_squared_error']])/len(scores_rf_tuned['test_neg_mean_squared_error']))\n",
    "\n",
    "# root mean squared log error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([(-1 * x) for x in scores_rf_tuned['train_neg_mean_absolute_error']])/len(scores_rf_tuned['train_neg_mean_absolute_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([(-1 * x) for x in scores_rf_tuned['test_neg_mean_absolute_error']])/len(scores_rf_tuned['test_neg_mean_absolute_error']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "plt.figure(figsize=[12,9])\n",
    "importances = list(pipeline_rf_tuned.steps[2][1].feature_importances_)\n",
    "\n",
    "imp_dict = {key: val for key, val in zip(list(X.columns), importances)}\n",
    "sorted_feats = sorted(imp_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "x_val = [x[0] for x in sorted_feats]\n",
    "y_val = [x[1] for x in sorted_feats]\n",
    "\n",
    "#importances.sort(reverse=True)\n",
    "sb.barplot(y_val, x_val)\n",
    "#importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing AdaBoost model with default hyperparameters\n",
    "model_ada = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "# creating and fitting the ML pipeline\n",
    "trend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
    "    ('model', model_ada)\n",
    "])\n",
    "\n",
    "pipeline_ada = pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a timeseries split of the datasets\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# doing cross validation on the chunks of data and calculating scores\n",
    "scores_ada = cross_validate(pipeline_ada, X, y, cv=time_split,\n",
    "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "                         return_train_score=True, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_ada['train_neg_mean_squared_error']])/len(scores_ada['train_neg_mean_squared_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_ada['test_neg_mean_squared_error']])/len(scores_ada['test_neg_mean_squared_error']))\n",
    "\n",
    "# root mean squared log error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_ada['train_neg_mean_absolute_error']])/len(scores_ada['train_neg_mean_absolute_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_ada['test_neg_mean_absolute_error']])/len(scores_ada['test_neg_mean_absolute_error']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the hyperparameters of AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of estimators in AdaBoost model\n",
    "n_estimators = randint(100, 1000)\n",
    "# learning rate\n",
    "learning_rate = uniform(0.001, 0.1)\n",
    "\n",
    "# Create the random grid\n",
    "random_grid_ada = {'model__n_estimators': n_estimators,\n",
    "                   'model__learning_rate': learning_rate}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the start time\n",
    "start_time = datetime.now()\n",
    "print(start_time)\n",
    "\n",
    "# instantiate and fit the randomizedsearch class to the random parameters\n",
    "rs_ada = RandomizedSearchCV(pipeline_ada, \n",
    "                        param_distributions=random_grid_ada, \n",
    "                        scoring='neg_mean_squared_error', \n",
    "                        n_jobs=-1,\n",
    "                        cv=time_split,\n",
    "                        n_iter = 5,\n",
    "                        verbose=10,\n",
    "                        random_state=40)\n",
    "rs_ada = rs_ada.fit(X, y)\n",
    "\n",
    "# print the total running time\n",
    "end_time = datetime.now()\n",
    "print('Total running time:', end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best number of \n",
    "rs_ada.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the best RandomForest model\n",
    "#pickle.dump(gs_ada.best_estimator_, open('adaboost.sav', 'wb'))\n",
    "\n",
    "# change the keys of the best_params dictionary to allow it to be used for the final model\n",
    "fix_best_params_ada = {key[7:]: val for key, val in rs_ada.best_params_.items()}\n",
    "\n",
    "print(fix_best_params_ada)\n",
    "\n",
    "# fit the randomforestregressor with the best_params as hyperparameters\n",
    "model_ada_tuned = AdaBoostRegressor(**fix_best_params_ada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and fitting the ML pipeline\n",
    "trend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
    "    ('model', model_ada_tuned)\n",
    "])\n",
    "\n",
    "pipeline_ada_tuned = pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a timeseries split of the datasets\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# doing cross validation on the chunks of data and calculating scores\n",
    "scores_ada_tuned = cross_validate(pipeline_ada_tuned, X, y, cv=time_split,\n",
    "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "                         return_train_score=True, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_error']])/len(scores_ada_tuned['train_neg_mean_squared_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_error']])/len(scores_ada_tuned['test_neg_mean_squared_error']))\n",
    "\n",
    "# root mean squared log error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([(-1 * x) for x in scores_ada_tuned['train_neg_mean_absolute_error']])/len(scores_ada_tuned['train_neg_mean_absolute_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([(-1 * x) for x in scores_ada_tuned['test_neg_mean_absolute_error']])/len(scores_ada_tuned['test_neg_mean_absolute_error']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify some values for hyperparameters around the values chosen by randomizedsearch\n",
    "grid_param_ada = {'model__learning_rate': [0.04, 0.045, 0.035],\n",
    "                  'model__n_estimators': [90, 100, 110]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gridsearch to search around the randomizedsearch best parameters and further improve the model\n",
    "gs_ada = GridSearchCV(pipeline_ada, \n",
    "                  param_grid=grid_param_ada, \n",
    "                  scoring='neg_mean_squared_error', \n",
    "                  verbose = 10,\n",
    "                  n_jobs=-1, \n",
    "                  cv=time_split)\n",
    "\n",
    "gs_ada = gs_ada.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the best RandomForest model\n",
    "pickle.dump(gs_ada.best_estimator_, open('adaboost.sav', 'wb'))\n",
    "\n",
    "# change the keys of the best_params dictionary to allow it to be used for the final model\n",
    "fix_best_params_ada = {key[7:]: val for key, val in gs_ada.best_params_.items()}\n",
    "\n",
    "print(fix_best_params_ada)\n",
    "\n",
    "# fit the randomforestregressor with the best_params as hyperparameters\n",
    "model_ada_tuned = AdaBoostRegressor(**fix_best_params_ada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and fitting the ML pipeline\n",
    "trend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
    "    ('model', model_ada_tuned)\n",
    "])\n",
    "\n",
    "pipeline_ada_tuned = pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a timeseries split of the datasets\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# doing cross validation on the chunks of data and calculating scores\n",
    "scores_ada_tuned = cross_validate(pipeline_ada_tuned, X, y, cv=time_split,\n",
    "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "                         return_train_score=True, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['train_neg_mean_squared_error']])/len(scores_ada_tuned['train_neg_mean_squared_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_ada_tuned['test_neg_mean_squared_error']])/len(scores_ada_tuned['test_neg_mean_squared_error']))\n",
    "\n",
    "# root mean squared log error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([(-1 * x) for x in scores_ada_tuned['train_neg_mean_absolute_error']])/len(scores_ada_tuned['train_neg_mean_absolute_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([(-1 * x) for x in scores_ada_tuned['test_neg_mean_absolute_error']])/len(scores_ada_tuned['test_neg_mean_absolute_error']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "plt.figure(figsize=[12,9])\n",
    "importances = list(pipeline_ada_tuned.steps[2][1].feature_importances_)\n",
    "\n",
    "imp_dict = {key: val for key, val in zip(list(X.columns), importances)}\n",
    "sorted_feats = sorted(imp_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "x_val = [x[0] for x in sorted_feats]\n",
    "y_val = [x[1] for x in sorted_feats]\n",
    "\n",
    "#importances.sort(reverse=True)\n",
    "sb.barplot(y_val, x_val)\n",
    "#importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing AdaBoost model with default hyperparameters\n",
    "model_xgb = xgb.XGBRegressor(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and fitting the ML pipeline\n",
    "trend_features = ['total_cust_mean'+str(window), 'total_cust_std'+str(window), 'total_cust_t-1']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('trend_diff', FunctionTransformer(np.log1p, validate=False), trend_features),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    #('debug', Debug()) # I have commented this out because it will hinder the execution of this pipeline\n",
    "    ('model', model_xgb)\n",
    "])\n",
    "\n",
    "pipeline_xgb = pipeline.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a timeseries split of the datasets\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# doing cross validation on the chunks of data and calculating scores\n",
    "scores_xgb = cross_validate(pipeline_xgb, X, y, cv=time_split,\n",
    "                         scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'],\n",
    "                         return_train_score=True, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_xgb['train_neg_mean_squared_error']])/len(scores_xgb['train_neg_mean_squared_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_xgb['test_neg_mean_squared_error']])/len(scores_xgb['test_neg_mean_squared_error']))\n",
    "\n",
    "# root mean squared log error\n",
    "print('Average root mean squared error train data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_xgb['train_neg_mean_absolute_error']])/len(scores_xgb['train_neg_mean_absolute_error']))\n",
    "print('Average root mean squared error test data:', \n",
    "      sum([np.sqrt(-1 * x) for x in scores_xgb['test_neg_mean_absolute_error']])/len(scores_xgb['test_neg_mean_absolute_error']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of estimators in AdaBoost model\n",
    "n_estimators = randint(100, 1000)\n",
    "# learning rate\n",
    "learning_rate = uniform(0.001, 0.1)\n",
    "\n",
    "# Create the random grid\n",
    "random_grid_ada = {'model__n_estimators': n_estimators,\n",
    "                   'model__learning_rate': learning_rate}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
